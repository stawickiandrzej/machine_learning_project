{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "788e907e",
   "metadata": {},
   "source": [
    "# Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ab2043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "del sys.modules[\"os\"]\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import types\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc2105",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8322f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.join(\"C:\\\\\", \"app\", \"python-scripts\", \"machine_learning\", \"project\")\n",
    "train_data_file = \"train_data.csv\"\n",
    "train_labels_file = \"train_labels.csv\"\n",
    "test_data_file = \"test_data.csv\"\n",
    "header_list = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149840b9",
   "metadata": {},
   "source": [
    "# Function definition's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5ab38c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddata(\n",
    "    dir_path: str, file_name: str, header_list: list\n",
    "    ) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    The function loads the given file and returns it as DataFrame\n",
    "    Args:\n",
    "        dir_path:   application working directory\n",
    "        file_name:  the name of file to be loaded\n",
    "        header_list:list of column names\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    return pd.read_csv(os.path.join(dir_path, file_name), names=header_list)\n",
    "\n",
    "def dump_file(_dir_path: str, _file_name: str, _buffer: pd.core.frame.DataFrame):\n",
    "    \"\"\"\n",
    "    The function saves the given variable buffer into binary file\n",
    "    Args:\n",
    "        _dir_path:   application working directorty\n",
    "        _file_name:  the name of file to be saved\n",
    "        _buffer:     variable to write\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    with open(os.path.join(_dir_path, _file_name), \"wb\") as f:\n",
    "        dump(_buffer, f)\n",
    "\n",
    "\n",
    "def load_file(_dir_path: str, _file_name: str) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    The function load dataset from binary file, and return pandas DataFrame\n",
    "    Args:\n",
    "        _dir_path:   application working directory\n",
    "        _file_name:  binary file to load\n",
    "\n",
    "    Returns: DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    with open(os.path.join(_dir_path, _file_name), \"rb\") as f:\n",
    "        return load(f)\n",
    "\n",
    "\n",
    "def standard_scaler(_df: pd.core.frame.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    standardization of dataset data using StandardScaler\n",
    "    Args:\n",
    "        _df:    dataset to standarization\n",
    "\n",
    "    Returns: Standardized features\n",
    "\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler().fit(_df)\n",
    "    return scaler.transform(_df)\n",
    "\n",
    "\n",
    "def min_max_scaler(_df: pd.core.frame.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    standardization of dataset data using MinMaxScaler\n",
    "    Args:\n",
    "        _df:    dataset to standarization\n",
    "\n",
    "    Returns: Standardized features\n",
    "\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    return scaler.fit_transform(_df)\n",
    "\n",
    "\n",
    "def log_transformation(df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    functions find and reduces skewed data with log-transformation\n",
    "    not used, because this dataset has normal distribution\n",
    "    Args:\n",
    "        _df: dataset to transform\n",
    "    Returns: DataFrame witnormalized data.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        df[str(col)].map(lambda i: np.log(i) if i > 0 else 0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def pca_data_rescaled(_np: np.ndarray, _n_comp: int, _svd_solver: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    the function reduces the size of the dataset \n",
    "    using the PCA method \n",
    "    Args:\n",
    "        _df: dataset to reduced\n",
    "    Returns: DataFrame with reduced size\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=_n_comp, svd_solver=_svd_solver)\n",
    "    pca.fit(_np)\n",
    "    return pca.transform(_np)\n",
    "\n",
    "\n",
    "def explain_variance_graph():\n",
    "    \"\"\"\n",
    "    plots the minimum number of components for 0.99 variances \n",
    "    Args: none\n",
    "    Returns: none\n",
    "    \"\"\"\n",
    "    %matplotlib inline\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    xi = np.arange(1, 3558, step=1)\n",
    "    y = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "    plt.ylim(0.0,1.1)\n",
    "\n",
    "    plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
    "\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative variance (%)')\n",
    "    plt.title('The number of components needed to explain variance')\n",
    "\n",
    "    plt.axhline(y=0.99, color='r', linestyle='-')\n",
    "    plt.text(0.5, 0.85, '99% cut-off threshold', color = 'red', fontsize=16)\n",
    "\n",
    "    ax.grid(axis='x')\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba39a36",
   "metadata": {},
   "source": [
    "# Main section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73a58130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading csv elapsed time: 18.633920907974243[sec]\n",
      "standarization time: 0.47403693199157715[sec]\n",
      "oryginal shape (3750, 10000)\n",
      "new shape (3750, 3557)\n",
      "reducing dataset size elapsed time: 85.14154243469238\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    # Load labels with list of headers into pandas datafame\n",
    "    header_list = [\"T0000\"]\n",
    "    df_targets = loaddata(dir_path, train_labels_file, header_list)\n",
    "\n",
    "    # Make dataset header\n",
    "    header_list = []\n",
    "    for i in range(10000):\n",
    "        header_list.append(f\"F{i:04d}\")\n",
    "\n",
    "    # Load dataset with list of headers into pandas dataframe\n",
    "    df_features = loaddata(dir_path, train_data_file, header_list)\n",
    "    print(f\"loading csv elapsed time: {time.time() - start_time}[sec]\")\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "    np_features = min_max_scaler(df_features)\n",
    "    print(f\"standarization time: {time.time() - start_time}[sec]\")\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(f'oryginal shape {np_features.shape}')\n",
    "    np_pca_reduced = pca_data_rescaled(np_features, 0.99, 'auto')\n",
    "    print(f'new shape {np_pca_reduced.shape}')\n",
    "    print(f\"reducing dataset size elapsed time: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e693980a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
