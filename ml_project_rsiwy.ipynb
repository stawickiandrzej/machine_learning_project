{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7108611f",
   "metadata": {},
   "source": [
    "# Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c49fa80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "del sys.modules[\"os\"]\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import types\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbef1b5",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cacab3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.join(\"C:\\\\\", \"app\", \"python-scripts\", \"machine_learning\", \"project\")\n",
    "train_data_file = \"train_data.csv\"\n",
    "train_labels_file = \"train_labels.csv\"\n",
    "test_data_file = \"test_data.csv\"\n",
    "header_list = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd45de27",
   "metadata": {},
   "source": [
    "# Function definition's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fe31de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddata(\n",
    "    dir_path: str, file_name: str, header_list: list\n",
    "    ) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    The function loads the given file and returns it as DataFrame\n",
    "    Args:\n",
    "        dir_path:   application working directory\n",
    "        file_name:  the name of file to be loaded\n",
    "        header_list:list of column names\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    return pd.read_csv(os.path.join(dir_path, file_name), names=header_list)\n",
    "\n",
    "def dump_file(_dir_path: str, _file_name: str, _buffer: pd.core.frame.DataFrame):\n",
    "    \"\"\"\n",
    "    The function saves the given variable buffer into binary file\n",
    "    Args:\n",
    "        _dir_path:   application working directorty\n",
    "        _file_name:  the name of file to be saved\n",
    "        _buffer:     variable to write\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    with open(os.path.join(_dir_path, _file_name), \"wb\") as f:\n",
    "        dump(_buffer, f)\n",
    "\n",
    "\n",
    "def load_file(_dir_path: str, _file_name: str) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    The function load dataset from binary file, and return pandas DataFrame\n",
    "    Args:\n",
    "        _dir_path:   application working directory\n",
    "        _file_name:  binary file to load\n",
    "\n",
    "    Returns: DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    with open(os.path.join(_dir_path, _file_name), \"rb\") as f:\n",
    "        return load(f)\n",
    "\n",
    "\n",
    "def standard_scaler(_df: pd.core.frame.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    standardization of dataset data using StandardScaler\n",
    "    Args:\n",
    "        _df:    dataset to standarization\n",
    "\n",
    "    Returns: Standardized features\n",
    "\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler().fit(_df)\n",
    "    return scaler.transform(_df)\n",
    "\n",
    "\n",
    "def min_max_scaler(_df: pd.core.frame.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    standardization of dataset data using MinMaxScaler\n",
    "    Args:\n",
    "        _df:    dataset to standarization\n",
    "\n",
    "    Returns: Standardized features\n",
    "\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    return scaler.fit_transform(_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972dbed4",
   "metadata": {},
   "source": [
    "# Main section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "236c7beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading csv elapsed time: 15.52216911315918\n",
      "standarization time: 0.48613905906677246\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    # Load labels with list of headers into pandas datafame\n",
    "    header_list = [\"T0000\"]\n",
    "    df_targets = loaddata(dir_path, train_labels_file, header_list)\n",
    "\n",
    "    # Make dataset header\n",
    "    header_list = []\n",
    "    for i in range(10000):\n",
    "        header_list.append(f\"F{i:04d}\")\n",
    "\n",
    "    # Load dataset with list of headers into pandas dataframe\n",
    "    df_features = loaddata(dir_path, train_data_file, header_list)\n",
    "    print(f\"loading csv elapsed time: {time.time() - start_time}[sec]\")\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "    np_features = min_max_scaler(df_features)\n",
    "    print(f\"standarization time: {time.time() - start_time}[sec]\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
